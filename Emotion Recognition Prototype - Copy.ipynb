{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import dlib\n",
    "import itertools\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "emotions = [\"happiness\", \"neutral\"] #Emotion list\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") #Or set this to whatever you named the downloaded file\n",
    "#clf = SVC(kernel='linear', probability=True, tol=1e-3)#, verbose = True) #Set the classifier as a support vector machines with polynomial kernel\n",
    "#clf = LogisticRegression(random_state=0)\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "data = {} #Make dictionary for all values\n",
    "#data['landmarks_vectorised'] = []\n",
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    for k,d in enumerate(detections): #For all detected face instances individually\n",
    "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        for i in range(1,68): #Store X and Y coordinates in two lists\n",
    "            xlist.append(float(shape.part(i).x)) \n",
    "            ylist.append(float(shape.part(i).y))\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(w)\n",
    "            landmarks_vectorised.append(z)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append((math.atan2(y, x)*360)/(2*math.pi))\n",
    "        data['landmarks_vectorised'] = landmarks_vectorised\n",
    "    if len(detections) < 1:\n",
    "        data['landmarks_vectorised'] = \"error\"\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        files = glob.glob(\"dataset\\\\%s\\\\*\" %emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-7\n",
    "        for item in files:\n",
    "            image = cv2.imread(item) #open image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            get_landmarks(clahe_image)\n",
    "            if data['landmarks_vectorised'] == \"error\":\n",
    "                print(\"no face detected on this one\")\n",
    "            else:\n",
    "                training_data.append(data['landmarks_vectorised']) #append image array to training data list\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "    return training_data, training_labels\n",
    "\n",
    "training_data, training_labels = make_sets()\n",
    "npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "clf.fit(npar_train, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "\t# take a bounding predicted by dlib and convert it\n",
    "\t# to the format (x, y, w, h) as we would normally do\n",
    "\t# with OpenCV\n",
    "\tx = rect.left()\n",
    "\ty = rect.top()\n",
    "\tw = rect.right() - x\n",
    "\th = rect.bottom() - y\n",
    " \n",
    "\t# return a tuple of (x, y, w, h)\n",
    "\treturn (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n"
     ]
    }
   ],
   "source": [
    "#Set up some required objects\n",
    "video_capture = cv2.VideoCapture(0) #Webcam object\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    cam_data = []\n",
    "    cam_labels = []\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    clahe_image = clahe.apply(gray)\n",
    "    get_landmarks(clahe_image)\n",
    "    if data['landmarks_vectorised'] == \"error\":\n",
    "        print(\"no face detected on this one\")\n",
    "    else:\n",
    "        cam_data.append(data['landmarks_vectorised']) #append image array to training data list\n",
    "        cam_arr = np.array(cam_data)\n",
    "        cam_labels = clf.predict(cam_arr)\n",
    "        faces = detector(gray, 1)\n",
    "        for (i, rect) in enumerate(faces):\n",
    "            # determine the facial landmarks for the face region, then convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "            # array\n",
    "            # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "            # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "            (x, y, w, h) = rect_to_bb(rect)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            # show the face number\n",
    "            cv2.putText(frame, \"{}\".format(emotions[cam_labels[i]]), (x - 10, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            # loop over the (x, y)-coordinates for the facial landmarks\n",
    "            # and draw them on the image\n",
    "            \n",
    "        cv2.imshow(\"Output\", frame)\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((68, 2), dtype=dtype)\n",
    " \n",
    "\t# loop over the 68 facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, 68):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    " \n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
