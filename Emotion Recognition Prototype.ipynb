{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from math import atan2,pi\n",
    "import dlib\n",
    "import numpy as np\n",
    "#import glob\n",
    "#import random\n",
    "#import itertools\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open('model.dat','rb')\n",
    "clf=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\"happiness\", \"neutral\"] #Emotion list\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "data = {} #Make dictionary for all values\n",
    "#data['landmarks_vectorised'] = []\n",
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    for k,d in enumerate(detections): #For all detected face instances individually\n",
    "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        for i in range(1,68): #Store X and Y coordinates in two lists\n",
    "            xlist.append(float(shape.part(i).x)) \n",
    "            ylist.append(float(shape.part(i).y))\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(w)\n",
    "            landmarks_vectorised.append(z)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append((atan2(y, x)*360)/(2*pi))\n",
    "        data['landmarks_vectorised'] = landmarks_vectorised\n",
    "    if len(detections) < 1:\n",
    "        data['landmarks_vectorised'] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Or set this to whatever you named the downloaded file\\n#clf = SVC(kernel=\\'linear\\', probability=True, tol=1e-3)#, verbose = True) #Set the classifier as a support vector machines with polynomial kernel\\n#clf = LogisticRegression(random_state=0)\\nclf = DecisionTreeClassifier(random_state=0)\\n\\ndef make_sets():\\n    training_data = []\\n    training_labels = []\\n    prediction_data = []\\n    prediction_labels = []\\n    for emotion in emotions:\\n        files = glob.glob(\"dataset\\\\%s\\\\*\" %emotion)\\n        #Append data to training and prediction list, and generate labels 0-7\\n        for item in files:\\n            image = cv2.imread(item) #open image\\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\\n            clahe_image = clahe.apply(gray)\\n            get_landmarks(clahe_image)\\n            if data[\\'landmarks_vectorised\\'] == \"error\":\\n                print(\"no face detected on this one\")\\n            else:\\n                training_data.append(data[\\'landmarks_vectorised\\']) #append image array to training data list\\n                training_labels.append(emotions.index(emotion))\\n    return training_data, training_labels\\n\\ntraining_data, training_labels = make_sets()\\nnpar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\\nclf.fit(npar_train, training_labels)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Or set this to whatever you named the downloaded file\n",
    "#clf = SVC(kernel='linear', probability=True, tol=1e-3)#, verbose = True) #Set the classifier as a support vector machines with polynomial kernel\n",
    "#clf = LogisticRegression(random_state=0)\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        files = glob.glob(\"dataset\\\\%s\\\\*\" %emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-7\n",
    "        for item in files:\n",
    "            image = cv2.imread(item) #open image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            get_landmarks(clahe_image)\n",
    "            if data['landmarks_vectorised'] == \"error\":\n",
    "                print(\"no face detected on this one\")\n",
    "            else:\n",
    "                training_data.append(data['landmarks_vectorised']) #append image array to training data list\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "    return training_data, training_labels\n",
    "\n",
    "training_data, training_labels = make_sets()\n",
    "npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "clf.fit(npar_train, training_labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "\t# take a bounding predicted by dlib and convert it\n",
    "\t# to the format (x, y, w, h) as we would normally do\n",
    "\t# with OpenCV\n",
    "\tx = rect.left()\n",
    "\ty = rect.top()\n",
    "\tw = rect.right() - x\n",
    "\th = rect.bottom() - y\n",
    " \n",
    "\t# return a tuple of (x, y, w, h)\n",
    "\treturn (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n",
      "no face detected on this one\n"
     ]
    }
   ],
   "source": [
    "#Set up some required objects\n",
    "video_capture = cv2.VideoCapture(0) #Webcam object\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    cam_data = []\n",
    "    cam_labels = []\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    clahe_image = clahe.apply(gray)\n",
    "    get_landmarks(clahe_image)\n",
    "    if data['landmarks_vectorised'] == \"error\":\n",
    "        print(\"no face detected on this one\")\n",
    "    else:\n",
    "        cam_data.append(data['landmarks_vectorised']) #append image array to training data list\n",
    "        cam_arr = np.array(cam_data)\n",
    "        cam_labels = clf.predict(cam_arr)\n",
    "        faces = detector(gray, 1)\n",
    "        for (i, rect) in enumerate(faces):\n",
    "            # determine the facial landmarks for the face region, then convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "            # array\n",
    "            # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "            # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "            (x, y, w, h) = rect_to_bb(rect)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            # show the face number\n",
    "            cv2.putText(frame, \"{}\".format(emotions[cam_labels[i]]), (x - 10, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            # loop over the (x, y)-coordinates for the facial landmarks\n",
    "            # and draw them on the image\n",
    "            \n",
    "        cv2.imshow(\"Output\", frame)\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
